# ---------------------------------------------------------
# 1. ã‚¤ãƒ³ãƒãƒ¼ãƒˆå‡¦ç†ã®æ•´ç†
# ---------------------------------------------------------
import os
import logging
import json
from datetime import datetime, time, timedelta
import io
import asyncio
import re

import discord
from discord.ext import commands, tasks
from discord import app_commands
import googlemaps
from geopy.distance import great_circle
from googleapiclient.http import MediaIoBaseDownload

# ---------------------------------------------------------
# ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨å®šæ•°è¨­å®š
# ---------------------------------------------------------
from config import JST
from utils.obsidian_utils import update_section

DATE_REGEX = re.compile(r'^\d{4}-\d{2}-\d{2}$')

ACTIVITY_TYPE_MAP = {
    "IN_PASSENGER_VEHICLE": "è»Šã§ã®ç§»å‹•",
    "WALKING": "å¾’æ­©ã§ã®ç§»å‹•",
    "CYCLING": "è‡ªè»¢è»Šã§ã®ç§»å‹•",
    "RUNNING": "ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°",
    "IN_BUS": "ãƒã‚¹ã§ã®ç§»å‹•",
    "IN_TRAIN": "é›»è»Šã§ã®ç§»å‹•",
    "IN_SUBWAY": "åœ°ä¸‹é‰„ã§ã®ç§»å‹•",
    "IN_TRAM": "è·¯é¢é›»è»Šã§ã®ç§»å‹•",
    "IN_FERRY": "ãƒ•ã‚§ãƒªãƒ¼ã§ã®ç§»å‹•",
    "FLYING": "é£›è¡Œæ©Ÿã§ã®ç§»å‹•",
    "STILL": "é™æ­¢",
    "UNKNOWN": "ä¸æ˜ãªç§»å‹•"
}

class LocationLogCog(commands.Cog):
    def __init__(self, bot: commands.Bot):
        self.bot = bot
        self.memo_channel_id = int(os.getenv("MEMO_CHANNEL_ID", 0))
        self.drive_folder_id = os.getenv("GOOGLE_DRIVE_FOLDER_ID")
        
        # ---------------------------------------------------------
        # çµ±åˆã•ã‚ŒãŸDriveã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆ©ç”¨
        # ---------------------------------------------------------
        self.drive_service = bot.drive_service
        
        self.home_coordinates = self._parse_coordinates(os.getenv("HOME_COORDINATES"))
        self.work_coordinates = self._parse_coordinates(os.getenv("WORK_COORDINATES"))
        self.exclude_radius_meters = int(os.getenv("EXCLUDE_RADIUS_METERS", 500))
        self.google_places_api_key = os.getenv("GOOGLE_PLACES_API_KEY")
        
        self.gmaps = googlemaps.Client(key=self.google_places_api_key) if self.google_places_api_key else None
        
        self.process_timeline_json.start()

    def cog_unload(self):
        self.process_timeline_json.cancel()

    # --- ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ ---
    def _get_place_name_from_id(self, place_id: str) -> str:
        if not self.gmaps: return f"å ´æ‰€ID: {place_id}"
        try:
            place_details = self.gmaps.place(place_id=place_id, language='ja')
            if place_details and 'result' in place_details and 'name' in place_details['result']:
                return place_details['result']['name']
        except Exception as e:
            logging.error(f"Places APIã‹ã‚‰ã®åå‰å–å¾—ã«å¤±æ•—: {e}")
        return f"å ´æ‰€ID: {place_id}"

    def _parse_coordinates(self, coord_str: str | None) -> tuple[float, float] | None:
        if not coord_str: return None
        try:
            lat, lon = map(float, coord_str.split(','))
            return (lat, lon)
        except (ValueError, TypeError):
            return None

    def _format_duration(self, duration_seconds: float) -> str:
        minutes = int(duration_seconds / 60)
        if minutes < 1: return "1åˆ†æœªæº€"
        hours, minutes = divmod(minutes, 60)
        if hours > 0: return f"{hours}æ™‚é–“{minutes}åˆ†"
        return f"{minutes}åˆ†"

    def _parse_iso_timestamp(self, ts_str: str) -> datetime | None:
        try:
            if ts_str.count(':') == 3:
                ts_str = ts_str[::-1].replace(':', '', 1)[::-1]
            return datetime.fromisoformat(ts_str)
        except (ValueError, TypeError): return None

    # --- Google Drive å›ºæœ‰å‡¦ç† (å°‚ç”¨ã®æ“ä½œã®ã¿æ®‹ã™) ---
    def _find_folder_in_root(self, service, name):
        query = f"'root' in parents and name = '{name}' and mimeType = 'application/vnd.google-apps.folder' and trashed = false"
        res = service.files().list(q=query, fields="files(id)").execute()
        files = res.get('files', [])
        return files[0]['id'] if files else None

    def _get_unprocessed_json(self, service, folder_id):
        query = f"'{folder_id}' in parents and name contains 'ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³.json' and not name contains 'å‡¦ç†æ¸ˆã¿_' and trashed = false"
        res = service.files().list(q=query, fields="files(id, name)").execute()
        return res.get('files', [])

    def _get_latest_timeline_json(self, service, folder_id):
        query = f"'{folder_id}' in parents and name contains 'ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³.json' and trashed = false"
        res = service.files().list(q=query, fields="files(id, name, createdTime)", orderBy="createdTime desc").execute()
        files = res.get('files', [])
        return files[0] if files else None

    def _rename_file(self, service, file_id, new_name):
        service.files().update(fileId=file_id, body={'name': new_name}).execute()

    def _read_json(self, service, file_id):
        fh = io.BytesIO()
        downloader = MediaIoBaseDownload(fh, service.files().get_media(fileId=file_id))
        done = False
        while not done: _, done = downloader.next_chunk()
        return json.loads(fh.getvalue().decode('utf-8'))

    # --- JSONãƒ‡ãƒ¼ã‚¿è§£æã®å…±é€šãƒ­ã‚¸ãƒƒã‚¯ ---
    def _extract_logs_from_json(self, data: dict, target_dates: set[str] = None) -> dict:
        """
        target_dates ã«å«ã¾ã‚Œã‚‹æ—¥ä»˜ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æŠ½å‡ºã™ã‚‹ã€‚
        """
        segments = data.get("semanticSegments", [])
        if not segments: return None

        events_by_date = {}
        for seg in segments:
            start_time = self._parse_iso_timestamp(seg.get("startTime", ''))
            end_time = self._parse_iso_timestamp(seg.get("endTime", ''))
            if not start_time or not end_time: continue

            event_date = start_time.astimezone(JST).date()
            date_str = event_date.strftime('%Y-%m-%d')

            # å¯¾è±¡æ—¥ä»˜ã‚»ãƒƒãƒˆãŒæŒ‡å®šã•ã‚Œã¦ã„ã¦ã€ã‹ã¤ãã‚Œã«å«ã¾ã‚Œãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if target_dates and date_str not in target_dates: continue

            events_by_date.setdefault(date_str, [])
            duration_seconds = (end_time - start_time).total_seconds()
            duration_formatted = self._format_duration(duration_seconds)
            event = {"start": start_time, "end": end_time}

            if (visit_data := seg.get("visit")):
                top_candidate = visit_data.get("topCandidate", {})
                lat_lng_str = top_candidate.get("placeLocation", {}).get("latLng")
                if not lat_lng_str: continue
                try:
                    lat_str, lon_str = lat_lng_str.replace('Â°', '').split(',')
                    place_coords = (float(lat_str), float(lon_str.strip()))
                except (ValueError, IndexError): continue
                
                place_name = "ä¸æ˜ãªå ´æ‰€"
                place_id = top_candidate.get('placeId')
                if place_id: place_name = self._get_place_name_from_id(place_id)
                
                if self.home_coordinates and great_circle(place_coords, self.home_coordinates).meters < self.exclude_radius_meters: place_name = "è‡ªå®…"
                elif self.work_coordinates and great_circle(place_coords, self.work_coordinates).meters < self.exclude_radius_meters: place_name = "å‹¤å‹™å…ˆ"
                
                event.update({"type": "stay", "name": place_name, "duration": duration_formatted})
                events_by_date[date_str].append(event)
            
            elif (activity_data := seg.get("activity")):
                activity_type = activity_data.get("topCandidate", {}).get("type", "UNKNOWN")
                distance_m = activity_data.get("distanceMeters", 0)
                distance_km_str = f" (ç´„{distance_m / 1000:.1f}km)" if distance_m > 0 else ""
                event.update({"type": "move", "activity": ACTIVITY_TYPE_MAP.get(activity_type, "ä¸æ˜ãªç§»å‹•"), "duration": duration_formatted, "distance": distance_km_str})
                events_by_date[date_str].append(event)

        # ãƒ†ã‚­ã‚¹ãƒˆæ•´å½¢
        logs_by_date = {}
        for d_str, events in sorted(events_by_date.items()):
            if not events: continue
            sorted_events = sorted(events, key=lambda x: x['start'])
            log_entries, last_place = [], None
            
            for event in sorted_events:
                start_str_jst = event['start'].astimezone(JST).strftime('%H:%M')
                if event['type'] == 'stay':
                    if last_place is not None: log_entries.append(f"- **{start_str_jst}** {event['name']}ã«åˆ°ç€")
                    log_entries.append(f"- **{start_str_jst} - {event['end'].astimezone(JST).strftime('%H:%M')}** ({event['duration']}) æ»åœ¨: {event['name']}")
                    last_place = event['name']
                elif event['type'] == 'move':
                    if last_place: log_entries.append(f"- **{start_str_jst}** {last_place}ã‚’å‡ºç™º")
                    log_entries.append(f"- **{start_str_jst} - {event['end'].astimezone(JST).strftime('%H:%M')}** ({event['duration']}) {event['activity']}{event['distance']}")
                    last_place = None
            
            logs_by_date[d_str] = "\n".join(log_entries)

        return logs_by_date

    # --- Obsidianæ›¸ãè¾¼ã¿ã®å…±é€šãƒ­ã‚¸ãƒƒã‚¯ ---
    async def _write_to_obsidian(self, date_str: str, log_text: str, force: bool = False) -> bool:
        """
        Obsidianã«æ›¸ãè¾¼ã‚€ã€‚ã™ã§ã«è¨˜å…¥æ¸ˆã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ãŒã€force=Trueã®å ´åˆã¯ä¸Šæ›¸ãã™ã‚‹ã€‚
        æ›¸ãè¾¼ã¿ã‚’å®Ÿè¡Œã—ãŸå ´åˆã¯Trueã‚’è¿”ã™ã€‚
        """
        service = self.drive_service.get_service()
        if not service: return False

        daily_folder = await self.drive_service.find_file(service, self.drive_folder_id, "DailyNotes")
        if not daily_folder:
            daily_folder = await self.drive_service.create_folder(service, self.drive_folder_id, "DailyNotes")

        daily_file = await self.drive_service.find_file(service, daily_folder, f"{date_str}.md")
        
        cur = ""
        if daily_file:
            try:
                cur = await self.drive_service.read_text_file(service, daily_file)
            except Exception as e:
                logging.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

        # â˜… å¤‰æ›´: å¼·åˆ¶ãƒ•ãƒ©ã‚°ãŒãªã„å ´åˆã€æ—¢ã«ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³å±¥æ­´ï¼ˆç®‡æ¡æ›¸ãã® - ï¼‰ãŒå­˜åœ¨ã™ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—
        if not force and re.search(r'## ğŸ“ Location History\s*-', cur):
            return False

        if not cur:
            cur = f"---\ndate: {date_str}\n---\n\n# Daily Note {date_str}\n\n## ğŸ“ Location History\n\n"
        
        new = update_section(cur, log_text, "## ğŸ“ Location History")
        
        if daily_file:
            await self.drive_service.update_text(service, daily_file, new)
        else:
            await self.drive_service.upload_text(service, daily_folder, f"{date_str}.md", new)
            
        return True


    # â–¼ æ¯æ—¥ 23:50 ã«å…¨è‡ªå‹•ã§å®Ÿè¡Œã•ã‚Œã‚‹å‡¦ç†
    @tasks.loop(time=time(hour=23, minute=50, tzinfo=JST))
    async def process_timeline_json(self):
        logging.info("ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³JSONã®è‡ªå‹•å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
        loop = asyncio.get_running_loop()
        service = self.drive_service.get_service()
        if not service: return

        channel = self.bot.get_channel(self.memo_channel_id)

        timeline_folder_id = await loop.run_in_executor(None, self._find_folder_in_root, service, "Timeline")
        if not timeline_folder_id: return

        json_files = await loop.run_in_executor(None, self._get_unprocessed_json, service, timeline_folder_id)
        if not json_files: return 

        # â˜… å¤‰æ›´: éå»ä½•æ—¥åˆ†ã‚’é¡ã£ã¦è£œå®Œå‡¦ç†ã™ã‚‹ã‹ã‚’æŒ‡å®šï¼ˆä»Šå›ã¯7æ—¥é–“ï¼‰
        lookback_days = 7
        today = datetime.now(JST).date()
        target_dates = { (today - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(lookback_days) }

        for file_info in json_files:
            file_id = file_info['id']
            file_name = file_info['name']
            
            try:
                data = await loop.run_in_executor(None, self._read_json, service, file_id)
            except Exception as e:
                logging.error(f"JSONèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                continue

            # éå»7æ—¥åˆ†ã®æ—¥ä»˜ã ã‘ã‚’æŠ½å‡º
            logs_by_date = self._extract_logs_from_json(data, target_dates=target_dates)

            processed_dates = []
            if logs_by_date:
                # æŠ½å‡ºã•ã‚ŒãŸæ—¥ä»˜ã”ã¨ã«æ›¸ãè¾¼ã¿ã‚’è©¦ã¿ã‚‹
                for date_str, log_text in logs_by_date.items():
                    # ã™ã§ã«è¨˜å…¥æ¸ˆã¿ã®æ—¥ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã€æœªå‡¦ç†ã®æ—¥ã ã‘æ›¸ãè¾¼ã¾ã‚Œã‚‹
                    was_written = await self._write_to_obsidian(date_str, log_text, force=False)
                    if was_written:
                        processed_dates.append(date_str)

            # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã€Œå‡¦ç†æ¸ˆã¿ã€ã«ãƒªãƒãƒ¼ãƒ 
            timestamp = datetime.now(JST).strftime('%Y%m%d_%H%M%S')
            await loop.run_in_executor(None, self._rename_file, service, file_id, f"å‡¦ç†æ¸ˆã¿_{timestamp}_{file_name}")
            
            if channel and processed_dates:
                # å‡¦ç†ã—ãŸæ—¥ä»˜ã‚’æ˜‡é †ã«ä¸¦ã¹æ›¿ãˆã¦é€šçŸ¥
                dates_str = ", ".join(sorted(processed_dates))
                await channel.send(f"ğŸ“ æœªå‡¦ç†ã®ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³å±¥æ­´ã‚’è§£æã—ã€ä»¥ä¸‹ã®æ—¥ä»˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’Obsidianã«ä¿å­˜ã—ã¾ã—ãŸï¼\n({dates_str})")


    # â–¼ æ‰‹å‹•ã§éå»ã®ãƒ‡ãƒ¼ã‚¿ã‚’åŒæœŸã™ã‚‹ã‚³ãƒãƒ³ãƒ‰
    @app_commands.command(name="location_sync", description="éå»ã®ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³å±¥æ­´ã‚’æŒ‡å®šã—ã¦æ‰‹å‹•ã§åŒæœŸã—ã¾ã™ã€‚")
    @app_commands.describe(target_date="åŒæœŸã—ãŸã„æ—¥ä»˜ (ä¾‹: 2026-02-15)")
    async def sync_location_manual(self, interaction: discord.Interaction, target_date: str):
        await interaction.response.defer(ephemeral=False)
        
        if not DATE_REGEX.match(target_date):
            await interaction.followup.send("âŒ æ—¥ä»˜ã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚(ä¾‹: 2026-02-15)")
            return

        loop = asyncio.get_running_loop()
        service = self.drive_service.get_service()
        if not service:
            await interaction.followup.send("âŒ Google Drive APIã®èªè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            return

        timeline_folder_id = await loop.run_in_executor(None, self._find_folder_in_root, service, "Timeline")
        if not timeline_folder_id:
            await interaction.followup.send("âŒ ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–ã« `Timeline` ãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return

        latest_file = await loop.run_in_executor(None, self._get_latest_timeline_json, service, timeline_folder_id)
        if not latest_file:
            await interaction.followup.send("âŒ `Timeline` ãƒ•ã‚©ãƒ«ãƒ€ã«JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return

        try:
            data = await loop.run_in_executor(None, self._read_json, service, latest_file['id'])
        except Exception as e:
            await interaction.followup.send(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚({e})")
            return

        # æ‰‹å‹•å®Ÿè¡Œã®å ´åˆã¯ã€æŒ‡å®šã•ã‚ŒãŸ1æ—¥åˆ†ã ã‘ã‚’æŠ½å‡º
        logs_by_date = self._extract_logs_from_json(data, target_dates={target_date})
        
        if not logs_by_date or target_date not in logs_by_date:
            await interaction.followup.send(f"âš ï¸ å‚ç…§ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ`{latest_file['name']}`ï¼‰å†…ã« **{target_date}** ã®ç§»å‹•ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return

        # æ‰‹å‹•å®Ÿè¡Œãªã®ã§ã€è¨˜å…¥æ¸ˆã¿ã§ã‚ã£ã¦ã‚‚å¼·åˆ¶çš„ã«ä¸Šæ›¸ã (force=True)
        await self._write_to_obsidian(target_date, logs_by_date[target_date], force=True)
        await interaction.followup.send(f"âœ… **{target_date}** ã®ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³å±¥æ­´ã‚’æ‰‹å‹•åŒæœŸã—ã¦ä¿å­˜ã—ã¾ã—ãŸï¼\n(å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«: `{latest_file['name']}`)")


    @process_timeline_json.before_loop
    async def before_process(self):
        await self.bot.wait_until_ready()

async def setup(bot): 
    await bot.add_cog(LocationLogCog(bot))